{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9baaa182",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "from datasets import DatasetDict, Dataset\n",
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee888a5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64721ce94b1447e49ed55e280972668c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df553fac",
   "metadata": {},
   "source": [
    "### Dataset load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea90e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load images and labels from a directory\n",
    "def load_data_from_dir(directory,label2id):\n",
    "    images = []\n",
    "    labels = []\n",
    "    image_ids = []\n",
    "    label_cat_dogs = [] # Assuming you have labels like 'cat' and 'dog'\n",
    "    \n",
    "    for dir in os.listdir(directory):\n",
    "        #print(label)\n",
    "        label_dir = os.path.join(directory, dir)\n",
    "        if os.path.isdir(label_dir):\n",
    "            for filename in os.listdir(label_dir):\n",
    "                if filename.endswith(\".jpg\"):\n",
    "                    img_path = os.path.join(label_dir, filename)\n",
    "                    img = Image.open(img_path)\n",
    "                    print(label)\n",
    "                    # if not isinstance(img, Image.JpegImageFile):\n",
    "                    #     img = img.convert(\"RGB\")  # Convert to RGB if necessary\n",
    "                    #     img = img.save(img_path, format='JPEG')  # Overwrite the image file\n",
    "                    #     img = Image.open(img_path)\n",
    "                    images.append(img)\n",
    "                    label = label2id[dir]\n",
    "                    labels.append(label)\n",
    "                    image_ids.append(dir)\n",
    "                    # Assuming you have some logic to convert label names to categorical values (e.g., 'cat' -> 0, 'dog' -> 1)\n",
    "                    label_cat_dogs.append(0 if label == 'cat' else 1)\n",
    "                    \n",
    "    return {\n",
    "        'image': images,\n",
    "        'label': labels,\n",
    "        'image_id': image_ids,\n",
    "        'label_cat_dog': label_cat_dogs\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d281d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir =  \"/home/dell/Desktop/DATASETS/agegender\"\n",
    "\n",
    "labels = os.listdir(os.path.join(data_dir, \"train\"))\n",
    "\n",
    "label2id = {label: i for i, label in enumerate(labels)}\n",
    "id2label = {i: label for i, label in enumerate(labels)}\n",
    "print(label2id,id2label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e7ea295",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data for training and testing\n",
    "train_data = load_data_from_dir(os.path.join(data_dir, \"train\"),label2id)\n",
    "test_data = load_data_from_dir(os.path.join(data_dir, \"val\"),label2id)\n",
    "\n",
    "# Create a DatasetDict\n",
    "dataset_dict = DatasetDict({\n",
    "    'train': Dataset.from_dict(train_data),\n",
    "    'test': Dataset.from_dict(test_data)\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9355ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a8acec",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dict['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4495dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "example = dataset_dict[\"train\"][0]\n",
    "example[\"label_cat_dog\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b971b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "example[\"image\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c49872f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dict[\"train\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a259b6bb",
   "metadata": {},
   "source": [
    "### Model load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfff9bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoImageProcessor, AutoModelForImageClassification\n",
    "\n",
    "model_name = \"facebook/dinov2-base\"\n",
    "processor = AutoImageProcessor.from_pretrained(model_name)\n",
    "model = AutoModelForImageClassification.from_pretrained(model_name, id2label=id2label, label2id=label2id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare dataset for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac37385",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import Compose, RandomResizedCrop, RandomHorizontalFlip, ColorJitter, ToTensor, Normalize\n",
    "import torch\n",
    "\n",
    "# make sure to use the appropriate image mean, std and interpolation\n",
    "# of the inference processor\n",
    "mean = processor.image_mean\n",
    "std = processor.image_std\n",
    "interpolation = processor.resample\n",
    "\n",
    "# for training, we use some image transformations from Torchvision\n",
    "# feel free to use other libraries like Albumentations or Kornia here\n",
    "train_transform = Compose([\n",
    "    RandomResizedCrop(size=(224, 224), scale=(0.08, 1.0), ratio=(0.75, 1.3333), interpolation=interpolation),\n",
    "    RandomHorizontalFlip(p=0.5),\n",
    "    ColorJitter(brightness=(0.6, 1.4), contrast=(0.6, 1.4), saturation=(0.6, 1.4)),\n",
    "    ToTensor(),\n",
    "    Normalize(mean=mean, std=std),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45cc6b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare(batch, mode=\"train\"):\n",
    "  # get images\n",
    "  images = batch[\"image\"]\n",
    "  #print(len(images))\n",
    "\n",
    "  # prepare for the model\n",
    "  if mode == \"train\":\n",
    "    images = [train_transform(image.convert(\"RGB\")) for image in images]\n",
    "    pixel_values = torch.stack(images)\n",
    "  elif mode == \"test\":\n",
    "    pixel_values = processor(images, return_tensors=\"pt\").pixel_values\n",
    "  else:\n",
    "    raise ValueError(f\"Mode {mode} not supported\")\n",
    "\n",
    "  inputs = {}\n",
    "  inputs[\"pixel_values\"] = pixel_values\n",
    "  inputs[\"labels\"] = torch.tensor(batch[\"label\"])\n",
    "\n",
    "  return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99763b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set num_proc equal to the number of CPU cores on your machine\n",
    "# see https://docs.python.org/3/library/multiprocessing.html#multiprocessing.cpu_count\n",
    "train_dataset = dataset_dict[\"train\"].map(prepare, num_proc=1, batched=True, batch_size=10, fn_kwargs={\"mode\":\"train\"})\n",
    "eval_dataset = dataset_dict[\"test\"].map(prepare, num_proc=1, batched=True, batch_size=10, fn_kwargs={\"mode\":\"test\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2994e739",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.set_format(\"torch\")\n",
    "eval_dataset.set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0d87bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset[0][\"pixel_values\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58db09d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset[0][\"labels\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a529908",
   "metadata": {},
   "source": [
    "### Train the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce2d940",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "# the compute_metrics function takes a Named Tuple as input:\n",
    "# predictions, which are the logits of the model as Numpy arrays,\n",
    "# and label_ids, which are the ground-truth labels as Numpy arrays.\n",
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"Computes accuracy on a batch of predictions\"\"\"\n",
    "    predictions = np.argmax(eval_pred.predictions, axis=1)\n",
    "    accuracy = accuracy_score(y_pred=predictions, y_true=eval_pred.label_ids)\n",
    "    return {\"accuracy\": accuracy}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a0d0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "args = TrainingArguments(\n",
    "    f\"{model_name}-finetuned-oxford\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    save_strategy = \"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=3,\n",
    "    warmup_ratio=0.1,\n",
    "    logging_steps=10,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    push_to_hub=True,\n",
    ")\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ccf63a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def collate_fn(examples):\n",
    "    pixel_values = torch.stack([example[\"pixel_values\"] for example in examples])\n",
    "    labels = torch.tensor([example[\"label\"] for example in examples])\n",
    "    return {\"pixel_values\": pixel_values, \"labels\": labels}\n",
    "\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=processor,\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator=collate_fn,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
